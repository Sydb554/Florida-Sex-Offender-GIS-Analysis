{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed09db5b",
   "metadata": {},
   "source": [
    "# Step 1: Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b4857",
   "metadata": {},
   "source": [
    "## 1a). Collecting and cleaning the Florida Sex Offender Registry Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f6c00",
   "metadata": {},
   "source": [
    "### Data Collection: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbeea9b",
   "metadata": {},
   "source": [
    "Data downloaded offender list from official Florida Law Enforcement Website.\n",
    " \n",
    "Florida Sex offender site: https://offender.fdle.state.fl.us/offender/sops/home.jsf\n",
    "Public data file info: https://www.fdle.state.fl.us/SOPS/public_data_file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73738b8a",
   "metadata": {},
   "source": [
    "### Data Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9612d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned successfully and saved to FloridaSOR_Clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define file path for downloaded dataset (source: https://www.fdle.state.fl.us/SOPS/public_data_file) \n",
    "# and the output file\n",
    "file_path = 'FloridaSOR.csv'\n",
    "output_file = 'FloridaSOR_Clean.csv'\n",
    "\n",
    "# create dataframe from downloaded date\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Error: The file {file_path} was not found.\")\n",
    "\n",
    "# define mappings for race and sex fields to standardize\n",
    "race_mapping = {\n",
    "    'W': 'White',\n",
    "    'B': 'Black or African American',\n",
    "    'I': 'American Indian or Alaska Native',\n",
    "    'U': 'Unknown or Unspecified',\n",
    "    'A': 'Asian'\n",
    "}\n",
    "\n",
    "sex_mapping = {\n",
    "    'M': 'Male',\n",
    "    'F': 'Female',\n",
    "    'U': 'Unknown or Unspecified'\n",
    "}\n",
    "\n",
    "# copy the original DataFrame to a new one for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# apply the mapping to race and sex columns to standardize\n",
    "df_clean['RACE'] = df['RACE'].map(race_mapping)\n",
    "df_clean['SEX'] = df['SEX'].map(sex_mapping)\n",
    "\n",
    "# concatenate address lines and remove any leading/trailing whitespace\n",
    "df_clean['PERM_ADDRESS'] = df[['PERM_ADDRESS_LINE_1', 'PERM_ADDRESS_LINE_2']].fillna('').agg(' '.join, axis=1).str.strip()\n",
    "df_clean['TEMP_ADDRESS'] = df[['TEMP_ADDRESS_LINE_1', 'TEMP_ADDRESS_LINE_2']].fillna('').agg(' '.join, axis=1).str.strip()\n",
    "df_clean['TRANS_ADDRESS'] = df[['TRANS_ADDRESS_LINE_1', 'TRANS_ADDRESS_LINE_2']].fillna('').agg(' '.join, axis=1).str.strip()\n",
    "\n",
    "# standardize city and county names by capitalizing the first letter of each word\n",
    "for col in ['PERM_CITY', 'TEMP_CITY', 'TRANS_CITY', 'PERM_COUNTY', 'TEMP_COUNTY', 'TRANS_COUNTY']:\n",
    "    df_clean[col] = df[col].str.title()\n",
    "\n",
    "# assign the state 'Florida' as the default for address-related state fields (to standardize, currently multiple versions)\n",
    "df_clean[['PERM_STATE', 'TEMP_STATE', 'TRANS_STATE']] = 'Florida'\n",
    "\n",
    "# function to format ZIP codes correctly by combining 5-digit and 4-digit parts\n",
    "def combine_zip(zip5, zip4):\n",
    "    if pd.isna(zip5):\n",
    "        return None\n",
    "    zip5_str = str(int(zip5))\n",
    "    zip4_str = str(int(zip4)).zfill(4) if pd.notna(zip4) else ''\n",
    "    return f\"{zip5_str}-{zip4_str}\" if zip4_str and zip4_str != '0000' else zip5_str\n",
    "\n",
    "# apply ZIP code formatting for permanent, temporary, and transitional addresses\n",
    "if {'PERM_ZIP', 'PERM_ZIP4'}.issubset(df.columns):\n",
    "    df_clean['PERM_ZIP'] = df.apply(lambda row: combine_zip(row['PERM_ZIP'], row['PERM_ZIP4']), axis=1)\n",
    "if {'TEMP_ZIP', 'TEMP_ZIP4'}.issubset(df.columns):\n",
    "    df_clean['TEMP_ZIP'] = df.apply(lambda row: combine_zip(row['TEMP_ZIP'], row['TEMP_ZIP4']), axis=1)\n",
    "if {'TRANS_ZIP', 'TRANS_ZIP4'}.issubset(df.columns):\n",
    "    df_clean['TRANS_ZIP'] = df.apply(lambda row: combine_zip(row['TRANS_ZIP'], row['TRANS_ZIP4']), axis=1)\n",
    "\n",
    "# rename columns for better readability\n",
    "df_clean.rename(columns={\n",
    "    'TRANS_ADDRESS_ADDED': 'Trans_Address_Added_Date',\n",
    "    'Department_of_Corrections_NUMBER': 'Department_of_Corrections_Number'\n",
    "}, inplace=True)\n",
    "\n",
    "# drop columns that won't be used in geospatial analysis\n",
    "columns_to_drop = ['SEX', 'RACE', 'PERM_STATE', 'TEMP_STATE', 'TRANS_STATE', 'PERM_ADDRESS_LINE_1', 'PERM_ADDRESS_LINE_2',\n",
    "                   'TEMP_ADDRESS_LINE_1', 'TEMP_ADDRESS_LINE_2', 'TRANS_ADDRESS_LINE_1', 'TRANS_ADDRESS_LINE_2']\n",
    "df_clean.drop(columns=[col for col in columns_to_drop if col in df_clean.columns], inplace=True)\n",
    "\n",
    "# save df to output file\n",
    "df_clean.to_csv(output_file, index=False)\n",
    "\n",
    "# done with data cleaning!\n",
    "print(f\"Data cleaned successfully and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8daa7b",
   "metadata": {},
   "source": [
    "## 1b). Collecting and cleaning Florida Public School Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811dc02",
   "metadata": {},
   "source": [
    "### Data Collection and Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d778c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully scraped and saved to FloridaPublicSchools_CLEAN.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# base URL for scraping the specific webpage for Florida schools\n",
    "base_url = \"https://web03.fldoe.org/Schools/schoolreport.asp?id=\"\n",
    "\n",
    "# dictionary mapping of school district IDs to their names\n",
    "districts = {\n",
    "    # mapping each ID to its corresponding district name\n",
    "    1: 'Alachua', 2: 'Baker', ... , 82: 'FSU Bay'\n",
    "}\n",
    "\n",
    "# dictionary to normalize the varied names of school types into a standard format\n",
    "school_type_mapping = {\n",
    "    'Universities': 'University',\n",
    "    'Community Colleges': 'Community College',\n",
    "    'High Schools': 'High School',\n",
    "    'Middle Schools': 'Middle School',\n",
    "    'Elementary Schools': 'Elementary School',\n",
    "    'Combination Schools': 'Combination School',\n",
    "    'Other Schools': 'Other School'\n",
    "}\n",
    "\n",
    "# initialize an empty list to store data for each school\n",
    "data = []\n",
    "\n",
    "# define a function to scrape data for a given school based on its ID and district name\n",
    "def scrape_data(school_id, district_name):\n",
    "    # fetch the webpage\n",
    "    response = requests.get(base_url + str(school_id))\n",
    "    # check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        return  # exit the function if the webpage is unreachable\n",
    "    \n",
    "    # parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # extract all table rows from the parsed HTML\n",
    "    rows = soup.find_all('tr')\n",
    "    school_type = None\n",
    "    \n",
    "    # process each row to extract relevant data\n",
    "    for row in rows[4:-1]:  # skip irrelevant rows\n",
    "        cells = row.text.strip().split('\\n')\n",
    "        if len(cells) == 1:\n",
    "            # update the school type based on the first cell if it's a standalone row\n",
    "            school_type = school_type_mapping.get(cells[0].strip(), cells[0].strip())\n",
    "        else:\n",
    "            # append a dictionary for each school containing detailed information\n",
    "            data.append({\n",
    "                'District': district_name,\n",
    "                'School Type': school_type,\n",
    "                'School Name': cells[0].strip(),\n",
    "                'Address': 'Unavailable' if 'Unavailable' in cells[2] else cells[2].strip(),\n",
    "                'Phone Number': cells[8].strip(),\n",
    "                'Public or Private': 'Public' #set all as Public because these are public schools\n",
    "            })\n",
    "\n",
    "# iterate over each district and scrape data using the predefined function\n",
    "for school_id, district_name in districts.items():\n",
    "    scrape_data(school_id, district_name)\n",
    "\n",
    "# done with data collection!\n",
    "# define output file\n",
    "output_file = 'FloridaPublicSchools_CLEAN.csv'\n",
    "\n",
    "# convert the list of dictionaries into a DataFrame for further manipulation and storage\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# manually update specific entries in the DataFrame for correctness\n",
    "df.update(pd.DataFrame([\n",
    "    {'District': 'Hillsborough', 'School Name': 'Dorothy C York Pk-8 Magnet School (0011)', \n",
    "     'Address': '5995 Covington Garden Dr, Apollo Beach, FL 33572', 'Phone Number': '(813) 533-2400'},\n",
    "    {'District': 'St. Johns', 'School Name': 'Lakeside Academy (0562)', \n",
    "     'Address': '1455 Twin Creeks Drive, St. Augustine, FL 32095', 'Phone Number': '(904) 547-4500'},\n",
    "    {'District': 'FSU Bay', 'School Name': 'Fsu Panama City District Office (9001)', \n",
    "     'Address': '4750 Collegiate Drive, Panama City, FL 32405', 'Phone Number': '(850) 872-4750'}\n",
    "]))\n",
    "\n",
    "# save df to output file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# done with data cleaning!\n",
    "print(f\"Data successfully scraped and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9734949",
   "metadata": {},
   "source": [
    "### 1c). Collecting and cleaning Florida Private School Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209f413",
   "metadata": {},
   "source": [
    "### Data Collectiong: \n",
    "Data downloaded from Florida Department of Eduation: https://www.floridaschoolchoice.org/information/privateschooldirectory/DownloadExcelFile.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea0ba6",
   "metadata": {},
   "source": [
    "### Data Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95314655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully cleaned and saved to FloridaPrivateSchools_CLEAN.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define file path for downloaded dataset (source: https://www.floridaschoolchoice.org/information/privateschooldirectory/DownloadExcelFile.aspx) \n",
    "# and the output file\n",
    "file_path = 'FloridaPrivateSchools.csv'\n",
    "output_file = 'FloridaPrivateSchools_CLEAN.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# fill missing values with empty strings to avoid processing errors in later steps\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# create a formatted address column by concatenating address parts and removing extra spaces\n",
    "df['Address'] = df[['Address 1', 'Address 2', 'City', 'State', 'Zip']].agg(lambda x: ' '.join(x).strip(), axis=1)\n",
    "\n",
    "# drop redundant address columns now that there is a combined and formatted address\n",
    "df.drop(columns=['Address 1', 'Address 2', 'City', 'State', 'Zip'], inplace=True)\n",
    "\n",
    "# standardize capitalization for district and school name columns to ensure consistency\n",
    "df[['District', 'School Name']] = df[['District', 'School Name']].apply(lambda x: x.str.title())\n",
    "\n",
    "# add a column indicating whether the school is public or private, set all as 'Private'\n",
    "df['Public or Private'] = 'Private'\n",
    "\n",
    "# save df to output file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# done wiht cleaning!\n",
    "print(f\"Data successfully cleaned and saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f45cf",
   "metadata": {},
   "source": [
    "### 1d). Collecting and Cleaning Florida Child Care Centers Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dba48",
   "metadata": {},
   "source": [
    "### Data Collections:\n",
    "Data downloaded from Florida Department of Children and Families: https://www.myflfamilies.com/search?q=%22child+care+provider+list%22#gsc.tab=0&gsc.q=%22child%20care%20provider%20list%22&gsc.page=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407405f7",
   "metadata": {},
   "source": [
    "### Data Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b97af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully cleaned and saved to FloridaChildCareCenters_CLEAN.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define file path for downloaded dataset (source: https://www.myflfamilies.com/search?q=%22child+care+provider+list%22#gsc.tab=0&gsc.q=%22child%20care%20provider%20list%22&gsc.page=1) \n",
    "# and the output file\n",
    "file_path = 'FloridaChildCareCenters.csv'\n",
    "output_file = 'FloridaChildCareCenters_CLEAN.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# rename column for consistency\n",
    "df.rename(columns={'Physical Address': 'Address'}, inplace=True)\n",
    "\n",
    "# drop columns not needed for geospatial analysis\n",
    "columns_to_drop = ['Physical Address', 'Is Head Start', 'Is Public School', 'Is School Age Only',\n",
    "                   'Is Urban Zoned', 'Facility Size', 'School Readiness Status', 'Gold Seal',\n",
    "                   'Fee', 'Is Faith Based', 'Is VPK', 'City', 'Zip', 'State', 'Director']\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# save df to output file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# done with cleaning!\n",
    "print(f\"Data successfully cleaned and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc4063",
   "metadata": {},
   "source": [
    "### 1e). Collecting and Cleaning Florida Public Parks Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ebea9",
   "metadata": {},
   "source": [
    "### Data Collection and Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf5b230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully scraped and saved to Florida_ParksWithPlaygrounds_CLEAN.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# base URL for scraping park information from Florida State Parks website\n",
    "base_url = \"https://www.floridastateparks.org/parks-and-trails?parks%5B0%5D=amenities%3A274&page=\"\n",
    "\n",
    "# storage list for collecting park data\n",
    "park_data = []\n",
    "\n",
    "# scrape park data across multiple pages\n",
    "for page in range(4):\n",
    "    response = requests.get(f\"{base_url}{page}\")\n",
    "    response.raise_for_status()  # check for HTTP request errors\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # find all park entries on the current page\n",
    "    for park in soup.find_all('div', class_='card__content'):\n",
    "        park_name = park.find('h3', class_='card__title').get_text(strip=True)\n",
    "        park_summary = park.find('div', class_='card__summary').get_text(strip=True)\n",
    "        park_address = park.find('div', class_='card__address').get_text(strip=True)\n",
    "        \n",
    "        # append each park's details to the park_data list\n",
    "        park_data.append({\n",
    "            'Park Name': park_name,\n",
    "            'Summary': park_summary,\n",
    "            'Address': park_address\n",
    "        })\n",
    "\n",
    "# create a DataFrame from the list of park data\n",
    "df = pd.DataFrame(park_data)\n",
    "output_file = 'Florida_ParksWithPlaygrounds_CLEAN.csv'\n",
    "df.to_csv(output_file, index=False) \n",
    "\n",
    "# done with data collection and cleaning! \n",
    "print(f\"Data successfully scraped and saved to {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
